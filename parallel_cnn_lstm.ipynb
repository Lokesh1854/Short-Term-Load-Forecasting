{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjRtvNC1gcivM7J5F0f+7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lokesh1854/Short-Term-Load-Forecasting/blob/main/parallel_cnn_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sFlB13CkF2f",
        "outputId": "03eb2641-a641-4961-99e0-d7b64d5ed13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, LSTM, concatenate, Dense, Dropout\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "rS18GxVikTJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, LSTM, concatenate, Dense, Dropout, Activation\n",
        "from keras.models import Model\n",
        "\n",
        "# LSTM input\n",
        "lstm_input = Input(shape=(48,1))  # Assuming input shape of (time_steps, features)\n",
        "\n",
        "# LSTM layers\n",
        "lstm_output_1 = LSTM(100, activation='relu', return_sequences=True)(lstm_input)\n",
        "lstm_output_2 = LSTM(100, activation='relu',)(lstm_output_1)\n",
        "lstm_output = Dense(1,activation='relu')(lstm_output_2)\n",
        "\n",
        "\n",
        "# CNN input\n",
        "cnn_input = Input(shape=(48,9))  # Assuming input shape of (additional_features, time_steps)\n",
        "\n",
        "# CNN layers\n",
        "\n",
        "conv_1 = Conv1D(filters=128, kernel_size=12, padding='same')(cnn_input)\n",
        "act_1 = keras.layers.Activation('relu') (conv_1)\n",
        "conv_2 = Conv1D(filters=64, kernel_size=6, strides=2, padding='same') (act_1)\n",
        "act_2 = keras.layers.Activation('relu') (conv_2)\n",
        "\n",
        "pool = keras.layers.GlobalAveragePooling1D() (act_2)\n",
        "\n",
        "dc = Dense(256,activation='relu') (pool)\n",
        "drop = Dropout(0.2)(dc)\n",
        "dc1 = Dense(128,activation='relu') (drop)\n",
        "dc2 = Dense(64,activation='relu') (dc1)\n",
        "dc3 = Dense(32,activation='relu') (dc2)\n",
        "dc4 = Dense(16,activation='relu') (dc3)\n",
        "dc5 = Dense(8,activation='relu') (dc4)\n",
        "dc6 = Dense(1,activation='relu')(dc5)\n",
        "\n",
        "# Concatenate LSTM and CNN outputs\n",
        "concatenated = concatenate([lstm_output, dc3])\n",
        "\n",
        "# Dense layers for combined output\n",
        "\n",
        "output_time_series = Dense(1, activation='relu')(concatenated)  # Output for LSTM\n",
        "\n",
        "# Define model with multiple inputs and multiple outputs\n",
        "model_p = Model(inputs=[lstm_input, cnn_input], outputs=[output_time_series])\n",
        "\n",
        "# Compile the model\n",
        "model_p.compile(optimizer='adam', loss=['mape'])\n",
        "\n",
        "# Print model summary\n",
        "model_p.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGUkI6UKkxGl",
        "outputId": "b977bc5d-bdc7-4100-de62-2a3f4e7bb23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_14 (InputLayer)       [(None, 48, 9)]              0         []                            \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)           (None, 48, 128)              13952     ['input_14[0][0]']            \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 48, 128)              0         ['conv1d_8[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)           (None, 24, 64)               49216     ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 24, 64)               0         ['conv1d_9[0][0]']            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 64)                   0         ['activation_5[0][0]']        \n",
            "  (GlobalAveragePooling1D)                                                                        \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 256)                  16640     ['global_average_pooling1d_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " input_13 (InputLayer)       [(None, 48, 1)]              0         []                            \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 256)                  0         ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_12 (LSTM)              (None, 48, 100)              40800     ['input_13[0][0]']            \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 128)                  32896     ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_13 (LSTM)              (None, 100)                  80400     ['lstm_12[0][0]']             \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 64)                   8256      ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 1)                    101       ['lstm_13[0][0]']             \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, 32)                   2080      ['dense_17[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 33)                   0         ['dense_14[0][0]',            \n",
            " )                                                                   'dense_18[0][0]']            \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 1)                    34        ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 244375 (954.59 KB)\n",
            "Trainable params: 244375 (954.59 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating time_steps of 1 day -- 24 x 6\n",
        "look_back = 2*24\n",
        "\n",
        "def prepare_cnn_data(data,look_back):\n",
        "  X,Y = [],[]\n",
        "  for i in range(len(data)-look_back):\n",
        "    X.append(np.array(data.iloc[i:i+look_back,:-1]))\n",
        "    Y.append(np.array(data.iloc[i+look_back,-1]))\n",
        "  return np.array(X),np.array(Y)"
      ],
      "metadata": {
        "id": "nra8X112k0FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "look_back = 2*24\n",
        "\n",
        "def prepare_lstm_data(data,look_back):\n",
        "  X_lstm,Y_lstm = [],[]\n",
        "  for i in range(len(data)-look_back):\n",
        "    X_lstm.append(np.array(data.iloc[i:i+look_back,-1]))\n",
        "    Y_lstm.append(np.array(data.iloc[i+look_back,-1]))\n",
        "  return np.array(X_lstm),np.array(Y_lstm)\n"
      ],
      "metadata": {
        "id": "pjw20MSEnL83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "n2XH_nkTnz9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_path = \"/content/drive/MyDrive/STLF/DataSet/Cluster 0\"\n",
        "\n",
        "mape_err = []\n",
        "\n",
        "for csvFile in os.listdir(cluster_path):\n",
        "  csvPath = os.path.join(cluster_path,csvFile)\n",
        "  df = pd.read_csv(csvPath)\n",
        "\n",
        "  time_attributes = pd.DataFrame(df['timestamp'])\n",
        "  time_attributes[\"timestamp\"] = pd.to_datetime(time_attributes[\"timestamp\"])\n",
        "\n",
        "  time_attributes[\"hour\"] = time_attributes[\"timestamp\"].dt.hour                          # represents hour of the day\n",
        "  time_attributes[\"minute\"] = time_attributes[\"timestamp\"].dt.minute                      # represents minute\n",
        "  time_attributes[\"minute\"] = time_attributes[\"minute\"] + (60*time_attributes[\"hour\"])\n",
        "\n",
        "  time_attributes.drop(\"timestamp\",axis=1,inplace=True)\n",
        "  time_attributes.drop(\"hour\",axis=1,inplace=True)\n",
        "\n",
        "  df = pd.concat([time_attributes,df],axis=1)\n",
        "  df.drop(\"timestamp\",axis=1,inplace=True)\n",
        "\n",
        "  X,Y = prepare_cnn_data(df,look_back)\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
        "\n",
        "  X_lstm,Y_lstm = prepare_lstm_data(df,look_back)\n",
        "  X_lstm_train, X_lstm_test, Y_lstm_train, Y_lstm_test = train_test_split(X_lstm, Y_lstm, test_size=0.1, random_state=42)\n",
        "\n",
        "  model_p.fit([X_lstm_train, X_train], Y_train , epochs=25, batch_size=32, validation_split=0.1)\n",
        "\n",
        "  mape_err.append([csvFile,model_p.evaluate([X_lstm_test,X_test],Y_test)])"
      ],
      "metadata": {
        "id": "0ZufZfiTnXhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc6ea593-152a-4a01-942f-bfbf205528ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "531/531 [==============================] - 76s 135ms/step - loss: 19.1949 - val_loss: 13.5773\n",
            "Epoch 2/25\n",
            "531/531 [==============================] - 58s 109ms/step - loss: 14.5731 - val_loss: 12.8479\n",
            "Epoch 3/25\n",
            "531/531 [==============================] - 53s 99ms/step - loss: 14.3384 - val_loss: 15.8469\n",
            "Epoch 4/25\n",
            "531/531 [==============================] - 52s 97ms/step - loss: 13.8520 - val_loss: 13.0424\n",
            "Epoch 5/25\n",
            "531/531 [==============================] - 52s 97ms/step - loss: 13.6716 - val_loss: 15.5683\n",
            "Epoch 6/25\n",
            "531/531 [==============================] - 50s 95ms/step - loss: 13.6798 - val_loss: 12.2360\n",
            "Epoch 7/25\n",
            "531/531 [==============================] - 49s 93ms/step - loss: 13.3038 - val_loss: 16.1237\n",
            "Epoch 8/25\n",
            "531/531 [==============================] - 50s 94ms/step - loss: 13.1098 - val_loss: 16.4362\n",
            "Epoch 9/25\n",
            "531/531 [==============================] - 49s 93ms/step - loss: 13.0955 - val_loss: 13.7230\n",
            "Epoch 10/25\n",
            "531/531 [==============================] - 50s 94ms/step - loss: 13.1061 - val_loss: 16.4041\n",
            "Epoch 11/25\n",
            "531/531 [==============================] - 50s 95ms/step - loss: 12.9821 - val_loss: 15.5940\n",
            "Epoch 12/25\n",
            "531/531 [==============================] - 49s 93ms/step - loss: 12.9380 - val_loss: 13.8430\n",
            "Epoch 13/25\n",
            "531/531 [==============================] - 49s 93ms/step - loss: 12.8448 - val_loss: 13.9615\n",
            "Epoch 14/25\n",
            "531/531 [==============================] - 49s 92ms/step - loss: 12.7004 - val_loss: 13.9714\n",
            "Epoch 15/25\n",
            "531/531 [==============================] - 48s 91ms/step - loss: 12.6692 - val_loss: 12.2170\n",
            "Epoch 16/25\n",
            "531/531 [==============================] - 48s 91ms/step - loss: 12.5067 - val_loss: 12.2928\n",
            "Epoch 17/25\n",
            "531/531 [==============================] - 53s 100ms/step - loss: 12.4122 - val_loss: 12.5742\n",
            "Epoch 18/25\n",
            "531/531 [==============================] - 48s 91ms/step - loss: 12.3853 - val_loss: 14.1254\n",
            "Epoch 19/25\n",
            "531/531 [==============================] - 48s 91ms/step - loss: 12.2759 - val_loss: 12.7802\n",
            "Epoch 20/25\n",
            "531/531 [==============================] - 48s 91ms/step - loss: 12.2338 - val_loss: 13.3690\n",
            "Epoch 21/25\n",
            "531/531 [==============================] - 48s 91ms/step - loss: 12.2540 - val_loss: 14.4905\n",
            "Epoch 22/25\n",
            "531/531 [==============================] - 49s 93ms/step - loss: 12.2677 - val_loss: 14.7601\n",
            "Epoch 23/25\n",
            "531/531 [==============================] - 50s 94ms/step - loss: 12.3093 - val_loss: 15.7312\n",
            "Epoch 24/25\n",
            "531/531 [==============================] - 48s 91ms/step - loss: 12.2267 - val_loss: 15.5400\n",
            "Epoch 25/25\n",
            "531/531 [==============================] - 49s 91ms/step - loss: 12.2161 - val_loss: 14.1816\n",
            "66/66 [==============================] - 2s 27ms/step - loss: 13.8172\n",
            "Epoch 1/25\n",
            "468/468 [==============================] - 42s 89ms/step - loss: 15.8382 - val_loss: 14.9118\n",
            "Epoch 2/25\n",
            "468/468 [==============================] - 44s 93ms/step - loss: 15.2401 - val_loss: 17.8616\n",
            "Epoch 3/25\n",
            "468/468 [==============================] - 44s 93ms/step - loss: 15.1419 - val_loss: 15.9154\n",
            "Epoch 4/25\n",
            "468/468 [==============================] - 44s 93ms/step - loss: 15.0988 - val_loss: 15.1195\n",
            "Epoch 5/25\n",
            "468/468 [==============================] - 42s 89ms/step - loss: 15.0765 - val_loss: 15.6236\n",
            "Epoch 6/25\n",
            "468/468 [==============================] - 43s 92ms/step - loss: 14.9229 - val_loss: 17.0132\n",
            "Epoch 7/25\n",
            "468/468 [==============================] - 43s 93ms/step - loss: 14.9708 - val_loss: 14.7616\n",
            "Epoch 8/25\n",
            "468/468 [==============================] - 43s 92ms/step - loss: 14.7698 - val_loss: 15.0597\n",
            "Epoch 9/25\n",
            "468/468 [==============================] - 43s 91ms/step - loss: 14.7132 - val_loss: 15.3750\n",
            "Epoch 10/25\n",
            "468/468 [==============================] - 42s 90ms/step - loss: 14.7211 - val_loss: 15.7601\n",
            "Epoch 11/25\n",
            "468/468 [==============================] - 43s 92ms/step - loss: 14.8599 - val_loss: 15.5973\n",
            "Epoch 12/25\n",
            "468/468 [==============================] - 45s 96ms/step - loss: 14.5483 - val_loss: 14.2162\n",
            "Epoch 13/25\n",
            "468/468 [==============================] - 43s 92ms/step - loss: 14.6576 - val_loss: 15.6807\n",
            "Epoch 14/25\n",
            "468/468 [==============================] - 42s 91ms/step - loss: 14.5761 - val_loss: 14.3795\n",
            "Epoch 15/25\n",
            "468/468 [==============================] - 42s 90ms/step - loss: 14.4963 - val_loss: 15.0703\n",
            "Epoch 16/25\n",
            "468/468 [==============================] - 43s 92ms/step - loss: 14.5821 - val_loss: 14.6589\n",
            "Epoch 17/25\n",
            "468/468 [==============================] - 43s 92ms/step - loss: 14.3676 - val_loss: 14.8476\n",
            "Epoch 18/25\n",
            "468/468 [==============================] - 43s 92ms/step - loss: 14.3434 - val_loss: 14.2582\n",
            "Epoch 19/25\n",
            "468/468 [==============================] - 41s 88ms/step - loss: 14.3503 - val_loss: 16.7181\n",
            "Epoch 20/25\n",
            "468/468 [==============================] - 44s 94ms/step - loss: 14.3760 - val_loss: 13.9900\n",
            "Epoch 21/25\n",
            "468/468 [==============================] - 44s 94ms/step - loss: 14.2952 - val_loss: 16.1366\n",
            "Epoch 22/25\n",
            "468/468 [==============================] - 44s 93ms/step - loss: 14.3189 - val_loss: 14.6115\n",
            "Epoch 23/25\n",
            "468/468 [==============================] - 43s 93ms/step - loss: 14.2304 - val_loss: 15.3615\n",
            "Epoch 24/25\n",
            "468/468 [==============================] - 42s 89ms/step - loss: 14.2451 - val_loss: 14.4686\n",
            "Epoch 25/25\n",
            "468/468 [==============================] - 43s 93ms/step - loss: 14.2060 - val_loss: 15.3949\n",
            "58/58 [==============================] - 2s 37ms/step - loss: 15.2825\n",
            "Epoch 1/25\n",
            "474/474 [==============================] - 44s 93ms/step - loss: 12.4338 - val_loss: 13.0510\n",
            "Epoch 2/25\n",
            "474/474 [==============================] - 44s 92ms/step - loss: 11.7089 - val_loss: 13.6911\n",
            "Epoch 3/25\n",
            "474/474 [==============================] - 42s 88ms/step - loss: 11.6848 - val_loss: 11.5659\n",
            "Epoch 4/25\n",
            "474/474 [==============================] - 43s 90ms/step - loss: 11.6898 - val_loss: 12.6003\n",
            "Epoch 5/25\n",
            "474/474 [==============================] - 44s 92ms/step - loss: 11.5910 - val_loss: 13.4481\n",
            "Epoch 6/25\n",
            "474/474 [==============================] - 44s 92ms/step - loss: 11.5686 - val_loss: 12.5829\n",
            "Epoch 7/25\n",
            "474/474 [==============================] - 43s 91ms/step - loss: 11.4322 - val_loss: 13.0321\n",
            "Epoch 8/25\n",
            "474/474 [==============================] - 43s 90ms/step - loss: 11.4434 - val_loss: 12.4043\n",
            "Epoch 9/25\n",
            "474/474 [==============================] - 42s 89ms/step - loss: 11.5118 - val_loss: 12.8879\n",
            "Epoch 10/25\n",
            "474/474 [==============================] - 44s 92ms/step - loss: 11.4310 - val_loss: 12.1924\n",
            "Epoch 11/25\n",
            "474/474 [==============================] - 44s 92ms/step - loss: 11.3794 - val_loss: 11.0126\n",
            "Epoch 12/25\n",
            "474/474 [==============================] - 43s 92ms/step - loss: 11.3699 - val_loss: 11.8185\n",
            "Epoch 13/25\n",
            "474/474 [==============================] - 42s 88ms/step - loss: 11.2799 - val_loss: 12.6485\n",
            "Epoch 14/25\n",
            "474/474 [==============================] - 43s 91ms/step - loss: 11.3161 - val_loss: 11.2126\n",
            "Epoch 15/25\n",
            "474/474 [==============================] - 44s 92ms/step - loss: 11.3276 - val_loss: 11.7590\n",
            "Epoch 16/25\n",
            "474/474 [==============================] - 43s 91ms/step - loss: 11.3199 - val_loss: 11.7731\n",
            "Epoch 17/25\n",
            "474/474 [==============================] - 42s 89ms/step - loss: 11.2142 - val_loss: 12.0704\n",
            "Epoch 18/25\n",
            "474/474 [==============================] - 43s 91ms/step - loss: 11.2430 - val_loss: 11.4529\n",
            "Epoch 19/25\n",
            "474/474 [==============================] - 44s 93ms/step - loss: 11.2185 - val_loss: 13.3902\n",
            "Epoch 20/25\n",
            "474/474 [==============================] - 44s 93ms/step - loss: 11.2195 - val_loss: 13.1784\n",
            "Epoch 21/25\n",
            "474/474 [==============================] - 44s 93ms/step - loss: 11.2367 - val_loss: 12.3518\n",
            "Epoch 22/25\n",
            "474/474 [==============================] - 43s 92ms/step - loss: 11.1746 - val_loss: 12.6670\n",
            "Epoch 23/25\n",
            "474/474 [==============================] - 44s 92ms/step - loss: 11.2202 - val_loss: 11.8308\n",
            "Epoch 24/25\n",
            "474/474 [==============================] - 48s 101ms/step - loss: 11.1093 - val_loss: 11.9738\n",
            "Epoch 25/25\n",
            "474/474 [==============================] - 47s 99ms/step - loss: 11.0738 - val_loss: 12.9353\n",
            "59/59 [==============================] - 2s 30ms/step - loss: 13.4160\n",
            "Epoch 1/25\n",
            "485/485 [==============================] - 49s 100ms/step - loss: 12.3693 - val_loss: 14.3138\n",
            "Epoch 2/25\n",
            "485/485 [==============================] - 49s 101ms/step - loss: 12.1424 - val_loss: 14.8174\n",
            "Epoch 3/25\n",
            "485/485 [==============================] - 48s 100ms/step - loss: 11.9975 - val_loss: 14.2972\n",
            "Epoch 4/25\n",
            "485/485 [==============================] - 47s 96ms/step - loss: 11.9823 - val_loss: 13.0568\n",
            "Epoch 5/25\n",
            "485/485 [==============================] - 48s 98ms/step - loss: 11.9350 - val_loss: 13.0134\n",
            "Epoch 6/25\n",
            "485/485 [==============================] - 49s 101ms/step - loss: 11.8739 - val_loss: 14.0609\n",
            "Epoch 7/25\n",
            "485/485 [==============================] - 48s 99ms/step - loss: 11.8444 - val_loss: 13.0873\n",
            "Epoch 8/25\n",
            "485/485 [==============================] - 48s 98ms/step - loss: 11.7778 - val_loss: 12.8036\n",
            "Epoch 9/25\n",
            "485/485 [==============================] - 47s 97ms/step - loss: 11.8336 - val_loss: 13.6126\n",
            "Epoch 10/25\n",
            "485/485 [==============================] - 48s 99ms/step - loss: 11.7052 - val_loss: 12.5467\n",
            "Epoch 11/25\n",
            "485/485 [==============================] - 47s 97ms/step - loss: 11.6954 - val_loss: 15.0005\n",
            "Epoch 12/25\n",
            "485/485 [==============================] - 47s 98ms/step - loss: 11.6488 - val_loss: 12.5402\n",
            "Epoch 13/25\n",
            "485/485 [==============================] - 47s 97ms/step - loss: 11.5699 - val_loss: 14.0048\n",
            "Epoch 14/25\n",
            "485/485 [==============================] - 46s 95ms/step - loss: 11.6794 - val_loss: 13.0165\n",
            "Epoch 15/25\n",
            "485/485 [==============================] - 46s 95ms/step - loss: 11.6414 - val_loss: 12.9693\n",
            "Epoch 16/25\n",
            "485/485 [==============================] - 46s 95ms/step - loss: 11.5693 - val_loss: 14.3273\n",
            "Epoch 17/25\n",
            "485/485 [==============================] - 46s 94ms/step - loss: 11.5002 - val_loss: 14.2865\n",
            "Epoch 18/25\n",
            "485/485 [==============================] - 46s 94ms/step - loss: 11.5534 - val_loss: 13.2407\n",
            "Epoch 19/25\n",
            "485/485 [==============================] - 45s 93ms/step - loss: 11.4470 - val_loss: 14.6712\n",
            "Epoch 20/25\n",
            "485/485 [==============================] - 46s 94ms/step - loss: 11.4468 - val_loss: 14.7882\n",
            "Epoch 21/25\n",
            "485/485 [==============================] - 46s 95ms/step - loss: 11.4073 - val_loss: 14.0317\n",
            "Epoch 22/25\n",
            "485/485 [==============================] - 47s 96ms/step - loss: 11.3389 - val_loss: 13.9217\n",
            "Epoch 23/25\n",
            "485/485 [==============================] - 46s 96ms/step - loss: 11.3197 - val_loss: 12.1468\n",
            "Epoch 24/25\n",
            "485/485 [==============================] - 46s 95ms/step - loss: 11.3730 - val_loss: 14.1505\n",
            "Epoch 25/25\n",
            "485/485 [==============================] - 46s 95ms/step - loss: 11.3286 - val_loss: 12.7513\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 12.5309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(cluster_path)"
      ],
      "metadata": {
        "id": "EiMYU2pvpnxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_err"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az5tr1W8_SgP",
        "outputId": "80f79609-dfc2-4a22-dca6-1adc2383a8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['block_16.csv', 13.817168235778809],\n",
              " ['block_27.csv', 15.282452583312988],\n",
              " ['block_4.csv', 13.416032791137695],\n",
              " ['block_9.csv', 12.530925750732422]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_p.save('cnn_lstm_parrallel')"
      ],
      "metadata": {
        "id": "g8cHbC08ozW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.read_csv('/content/drive/MyDrive/STLF/DataSet/modified_dataset/block_13.csv')\n",
        "\n",
        "time_attributes = pd.DataFrame(new_df['timestamp'])\n",
        "time_attributes[\"timestamp\"] = pd.to_datetime(time_attributes[\"timestamp\"])\n",
        "\n",
        "time_attributes[\"hour\"] = time_attributes[\"timestamp\"].dt.hour                          # represents hour of the day\n",
        "time_attributes[\"minute\"] = time_attributes[\"timestamp\"].dt.minute                      # represents minute\n",
        "time_attributes[\"minute\"] = time_attributes[\"minute\"] + (60*time_attributes[\"hour\"])\n",
        "\n",
        "time_attributes.drop(\"timestamp\",axis=1,inplace=True)\n",
        "time_attributes.drop(\"hour\",axis=1,inplace=True)\n",
        "\n",
        "new_df = pd.concat([time_attributes,new_df],axis=1)\n",
        "new_df.drop(\"timestamp\",axis=1,inplace=True)\n",
        "\n",
        "X_new,Y_new = prepare_cnn_data(new_df,look_back)\n",
        "X_new_lstm,Y_new_lstm = prepare_lstm_data(new_df,look_back)\n",
        "model_p.evaluate([X_new_lstm,X_new],Y_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XKTEew1RyGO",
        "outputId": "da0d5775-8ca2-4554-8578-92dd21d92a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "581/581 [==============================] - 18s 30ms/step - loss: 14.0335\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.033499717712402"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}